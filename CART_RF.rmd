---
title: "CART_RF"
output: html_document
---

# CART & Random Forest
```{r}
install.packages("yardstick")
```


```{r setup, include=FALSE}
library(mlbench)
library(rpart)
library(rattle)
library(yardstick)
library(randomForest)
library(xtable)
data("Ionosphere")
```
According to the documentation, this data set include "[...] radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. [...] The targets were free electrons in the ionosphere. "good" radar returns are those showing evidence of some type of structure in the ionosphere. "bad" returns are those that do not; their signals pass through the ionosphere." [source](https://www.rdocumentation.org/packages/mlbench/versions/2.1-3/topics/Ionosphere).

## EDA
```{r}
summary(Ionosphere)
```
Based on the "V" variables predictors representing values of the measured electromagnetic field (with both real and complex components), we'll build a classification and then a Random Forest model in order to predict the *Class* variable about whether a measure is "good" or "bad" (refer to the above description for an interpretation of those categories).

From the initial exploration, we can see that the dataset contains almost twice observations having "class"=good as observation having "class"=bad. Therefore, we'll have to be careful of this characteristic When drawing conclusions about our models.

```{r}
myblue <- rgb(0, 0, 255, max = 255, alpha = 125, names = "blue50")
myred <- rgb(255, 0, 0, max = 255, alpha = 125, names = "red50")
hist(Ionosphere$V3, col=myblue, alpha=125)
hist(Ionosphere$V4, col=myred, alpha=125, add=TRUE)
```


## Dataset preparation
a) To build the Random Forest model, we'll do a random 80/20 train/test split on our dataset.
```{r}
nelem = nrow(Ionosphere)
ntrain = round(0.8 * nelem, 0)
ntest = nelem - ntrain

train.index = sort(sample(x=1:nelem, size=ntrain, replace=FALSE))
test.index = setdiff(1:nelem, train.index)

Ionosphere.train = Ionosphere[train.index,]
Ionosphere.test = Ionosphere[test.index,]
```

Before going forward with this split, we need to ensure a good representation of both classes in our split (full set, train and test set):
```{r}
train_test_split = rbind(summary(Ionosphere$Class),
summary(Ionosphere.train$Class),
summary(Ionosphere.test$Class))

print(train_test_split)

print(xtable(train_test_split, type="latex"), file="train_test_split.tex")
```


## Classification tree
### First iteration
As our first model, we will apply a simple classification tree on the training data then we'll see how it performs and the test set.
```{r, echo=FALSE}
# To get variables names to sum
for(v in colnames(Ionosphere)[-35]){cat(paste(v,"+ "))}
```

```{r}
Ionosphere.tree.train = rpart(formula = Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34, data = Ionosphere.train, method="class")
```

**Answer b)** Here is the classification tree:
```{r}
fancyRpartPlot(Ionosphere.tree.train, main="Default parameter run")
```
```{r}
Ionosphere.tree.train
```


Let's describe the obtained classification tree. We got 5 splits and 6 leaves.
Although having 34 variables to predict the "class" variable, only 5 (V5, V27, V8, V22 and V3) are selected for this classification tree.


### Performance test
Now, we can apply this classification tree to the test dataset and conclude about its performances.
```{r}
pred = predict(Ionosphere.tree.train, Ionosphere.test, type="class")
before_cp_a <- mean(pred==Ionosphere.test$Class)
before_cp_a
```
With a (simple) accuracy score, we have 0.87% of correct predictions on the tests, which is not bad for a first iteration.

**Answer d)** Therefore, we have a $1-0.87 = 0.13\%$ of misclassified observations which can also be seen under a confusion matrix:
```{r}
table(pred=pred, true=Ionosphere.test$Class)
```
But as we said before, there is a consequent disparity in the repartition of "good"/"bad" observation. To complete our simple accuracy score, we'll compute the balanced accuracy score, which takes into account the repartition of the data.
```{r}
before_cp_ba <- bal_accuracy_vec(pred, truth=Ionosphere.test$Class, estimator="binary")
before_cp_ba
```
The balanced accuracy being lower than the simple accuracy mean that the least represented class (which is "bad" here) is having a stronger weight (in fact, as weighted as the other class) in the accuracy score. 

Thus, we can deduce that predictions are better on class="good" observation than class="bad". 

Finally, we can confirm this conclusion with the confusion matrix showing that only 3 observations were misclassified as good (when supposed to be bad) when 6 observations, **twice more**, were misclassified as bad (when supposed to be good).

### Improving our classification tree with pruning
Before moving forward to the random forest, could we improve our classification tree using pruning ?

```{r}
plotcp(Ionosphere.tree.train, main="Error related to CP values")
```

To so, we'll acquire the complexity parameter (CP) leading to the lowest error:
```{r}
min_iono <- which.min(Ionosphere.tree.train$cptable[, "xerror"])
opt_cp <- Ionosphere.tree.train$cptable[min_iono, "CP"]
opt_cp
```
Which is corresponding to a CP value of 3 when our initial classification tree includes 5 splits instead of 3. Let's recompute the classification tree using this CP parameter and compute the accuracy score:
```{r}
Ionosphere.tree.train = rpart(formula = Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34, data = Ionosphere.train, method="class", cp=opt_cp)
pred = predict(Ionosphere.tree.train, Ionosphere.test, type="class")
after_cp_a <- mean(pred==Ionosphere.test$Class)
after_cp_a
```
and the balanced accuracy:
```{r}
after_cp_ba <- bal_accuracy_vec(pred, truth=Ionosphere.test$Class, estimator="binary")
after_cp_ba
```
Which are both better than before.

## Random forest
### Default parameters
Now we will compute a random forest in the hope for a better accuracy score, using the `randomForest` library:
```{r}
rf = randomForest(formula = Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34, data = Ionosphere.train)
print(rf)
```
### Tree numbers
```{r}
plot(rf, main="Random forest")
```

We can see that our error rates seems lower around 130. We will generate a new Random Forest model:
```{r}
rf = randomForest(formula = Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34, data = Ionosphere.train, ntree=130)
plot(rf)
```


We then use this first Random Forest to predict the test data:
```{r}
pred = predict(rf, Ionosphere.test, type="class")
table(pred=pred, true=Ionosphere.test$Class)
```
We still have a more "good" values that were supposed to be classified as "bad" than the contrary but we have a way better accuracy (whether it is simple or balanced):
```{r}
paste(mean(pred==Ionosphere.test$Class), bal_accuracy_vec(pred, truth=Ionosphere.test$Class, estimator="binary"))
```

### Optimal number of variables
Now that we have fixed the number of trees, we will try to find the optimal number of variable used in our trees. To do so, we will iterate over a range of variable number and plot the OOB error rate.
```{r}
oob_per_mtry <- function(mtry){
  model = randomForest(formula = Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34, data = Ionosphere.train, ntree=1000, mtry=mtry)
  
  return(model$err.rate[998,1])
}

mtry_vals = c(1:34)
oob_vals = lapply(mtry_vals, oob_per_mtry)
plot(mtry_vals, oob_vals)
```

On the above plot we can see that the OOB error seems to be reaching a global minimum around 7 variables. However, we have to take into consideration that those result we obtained one set of train/test split and parameters may differ for different simulations.

We'll then recompute our Random Forest model with this *mtry* paramter:
```{r}
model = randomForest(formula = Class ~ V1 + V2 + V3 + V4 + V5 + V6 + V7 + V8 + V9 + V10 + V11 + V12 + V13 + V14 + V15 + V16 + V17 + V18 + V19 + V20 + V21 + V22 + V23 + V24 + V25 + V26 + V27 + V28 + V29 + V30 + V31 + V32 + V33 + V34, data = Ionosphere.train, ntree=1000, mtry=7)
print(rf)
```

**Answer e)** Which helps us reach a an OOB error rate of 7.47% with this optimal (footnote of relative ...) of parameters: $$m_try = 7, ntree=1000$$.

## Going further with this model
From a purely physical point of view, we could also take into account the norm of the electromagnetic field into consideration to state whether a signal is "good" or "bad". Therefore, we'll add the norm of each couple of measurement (real and complex values) and study its influence on the Random Forest performances.

```{r}
Ionosphere$V3 = as.numeric(Ionosphere$V3)
Ionosphere$V4 = as.numeric(Ionosphere$V4)

Ionosphere$P1 = (Ionosphere$V3)**2 + (Ionosphere$V4)**2
```


TODO : even partitions with createDataPartitions from caret


TODO : add a norm variable + which are complex and real values ?

